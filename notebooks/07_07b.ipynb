{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7252adbf-4476-49ed-b1bb-c51604a2e729",
   "metadata": {},
   "source": [
    "## Asynchronous scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc56e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aiohttp -q\n",
    "!pip install asyncio -q\n",
    "!pip install nest-asyncio -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb191576-cfe0-4823-aeb8-2ce0ed7a1cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba5df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e34d89",
   "metadata": {},
   "source": [
    "async means that the function can be paused and resumed at any time. This is useful when we are waiting for a response from a server. We can pause the function and do other things while waiting for the response. When the response is ready, we can resume the function.\n",
    "\n",
    "It works in parallel with other functions. This means that we can run multiple functions at the same time. This is useful when we are scraping multiple pages. We can scrape multiple pages at the same time, which makes the scraping process faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23e0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrap_and_save_links(text):\n",
    "    \"\"\"\n",
    "    Process the HTML text and save the links to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        text (str): The HTML content to process.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(text, 'html.parser') # parse the html\n",
    "    file = open('csv_file', 'a', newline='') # open the file in append mode\n",
    "    writer= csv.writer(file, delimiter=',') # create a csv writer object\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile('^http')}): # find all the links that start with http\n",
    "        link = link.get('href') # get the link\n",
    "        writer.writerow([link]) # write the link to the csv file\n",
    "    file.close() # close the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4f9d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch(session, url): # function to fetch the url\n",
    "   \"\"\"\n",
    "   Fetch the content of the URL and process it.\n",
    "\n",
    "   Args:\n",
    "       session (aiohttp.ClientSession): The aiohttp client session.\n",
    "       url (str): The URL to fetch.\n",
    "   \"\"\"\n",
    "   try: # try to fetch the url\n",
    "      async with session.get(url) as response: # get the response from the url without blocking the event loop or other processes\n",
    "         text= await response.text() # get the text from the response by using await for waiting for the response\n",
    "         task = asyncio.create_task(scrap_and_save_links(text)) # create a task to scrap and save the links\n",
    "         await task # wait for the task (scrap_and_save_links) to complete\n",
    "   except Exception as e: # if there is an exception\n",
    "      print(str(e)) # print the exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b996781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrap(urls): # function to scrap the urls\n",
    "  \"\"\"\n",
    "  Scrape the provided URLs.\n",
    "\n",
    "  Args:\n",
    "      urls (list): A list of URLs to scrape.\n",
    "  \"\"\"\n",
    "  tasks = []\n",
    "  async with aiohttp.ClientSession() as session: # create a client session for whole HTTP request\n",
    "    for url in urls: # for each url in the list of urls\n",
    "      tasks.append(fetch(session,url)) # append the task to fetch the url\n",
    "    await asyncio.gather(*tasks) # gather all the tasks - gather is used to wait for all the tasks to complete in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f7fb591",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://analytics.usa.gov/', 'https://www.python.org/', 'https://www.linkedin.com/']\n",
    "asyncio.run(scrap(urls=urls))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
